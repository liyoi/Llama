{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LLamaOnlyPERO\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# from data import getDataLoader, getVocabSize\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载完毕\n"
     ]
    }
   ],
   "source": [
    "from NewsDataLoader import getABatch, getVocabSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embedding: int = 360  # 嵌入维度\n",
    "# 注意力相关参数\n",
    "n_heads: int = 4  # 注意力头\n",
    "head_dim: int = n_embedding // n_heads  # 每个注意力头的维度\n",
    "vocab_size: int = -1  # 词表大小\n",
    "multiple_of: int = 4  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "batch_size: int = 128  # 一个批量大小\n",
    "block_size: int = 512  # 一个批量中包含的字符数\n",
    "dropout: int = 0.2\n",
    "device: str = 'cuda:5' if torch.cuda.is_available() else 'cpu'\n",
    "#device=\"cpu\"\n",
    "max_iter: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建logger对象\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建FileHandler并设置日志格式、保存路径等参数\n",
    "file_handler = logging.FileHandler('log', mode='w')\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# 添加FileHandler到logger对象\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y in getABatch('val', batch_size, block_size):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    out = np.mean(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "vocab_size = getVocabSize()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):\n",
    "    # 计算词向量元素两两分组之后，每组元素对应的旋转角度\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 生成 token 序列索引 t = [0, 1,..., seq_len-1]\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    # freqs.shape = [seq_len, dim // 2]\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # torch.polar 的文档\n",
    "    # https://pytorch.org/docs/stable/generated/torch.polar.html\n",
    "    # 计算结果是个复数向量\n",
    "    # 假设 freqs = [x, y]\n",
    "    # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "# PERO旋转位置嵌入\n",
    "def apply_rotary_emb(\n",
    "        xq: torch.Tensor,\n",
    "        xk: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # xq.shape = [batch_size, seq_len, dim]\n",
    "    # xq_.shape = [batch_size, seq_len, dim // 2, 2]\n",
    "    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2).to(device)\n",
    "    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2).to(device)\n",
    "\n",
    "    # 转为复数域\n",
    "    xq_ = torch.view_as_complex(xq_)\n",
    "    xk_ = torch.view_as_complex(xk_)\n",
    "    # 应用旋转操作，然后将结果转回实数域\n",
    "    # xq_out.shape = [batch_size, seq_len, dim]\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" 掩藏注意力 \"\"\"\n",
    "    freqs_cis = None\n",
    "\n",
    "    def __init__(self, head_size, n_embedding=360, block_size=256, dropout=0.2, pos_embed_method=\"rope\"):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pos_embed_method = pos_embed_method\n",
    "        self.freqs_cis=precompute_freqs_cis(dim=head_size, seq_len=block_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        if self.pos_embed_method == 'rope':\n",
    "            # Reformer相对位置编码\n",
    " #           if self.freqs_cis is None:\n",
    " #               self.freqs_cis = precompute_freqs_cis(dim=key.shape[-1], seq_len=T).to(key.device)\n",
    "            xq, xk = apply_rotary_emb(key, query, self.freqs_cis)    \n",
    "            query, key = xq, xk\n",
    "            # key*value/(d**-0.5)\n",
    "        wei = key @ query.transpose(-2, -1) * (key.shape[-1] ** -0.5)\n",
    "        # 掩藏操作，使注意力只能看到前面数据\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, -1)\n",
    "        wei = self.dropout(wei)\n",
    "        value = self.value(x)\n",
    "        outputs = wei @ value\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" 多头注意力 \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, block_size=256,n_embedding=360, pos_embed_method=\"rope\"):\n",
    "        super().__init__()\n",
    "        # 多头注意力由多个注意力叠加\n",
    "        self.heads = nn.ModuleList([Head(head_size,n_embedding=360, block_size=256,pos_embed_method=pos_embed_method) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(n_embedding, n_embedding)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 拼接各个注意力的输出结果\n",
    "        output = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        output = self.dropout(self.linear(output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\" RMSNorm均方层归一化 \"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, eps: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_embedding = n_emb\n",
    "        self.weights = nn.Parameter(torch.ones(n_emb))\n",
    "        self.eps = eps\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x / torch.sqrt(torch.mean(torch.square(x), dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        o = self._norm(x)\n",
    "        return o * self.weights\n",
    "\n",
    "\n",
    "def _Swish(x, beta=1):\n",
    "    return x * (1 / 1 + torch.exp(-beta * x))\n",
    "\n",
    "\n",
    "class SwishGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, beta=1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beta = beta\n",
    "        self.w1 = nn.Linear(dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return _Swish(self.w1(x), self.beta) * self.w2(x)\n",
    "\n",
    "\n",
    "class FeedForwardWithRELU(nn.Module):\n",
    "    \"\"\" 前馈神经网络 \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, hidden_dim, dropout: float):\n",
    "        super().__init__()\n",
    "        self.w3 = nn.Linear(dim, hidden_dim)\n",
    "        self.swish = nn.ReLU()\n",
    "        self.w4 = nn.Linear(hidden_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w4(self.swish(self.w3(x))))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" 前馈神经网络 \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int = 4, dropout: float = 0.2, mode='swish'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        if self.mode == 'swish':\n",
    "            # 4*(2*3*hidden_dim/4) 缩小为2*3倍\n",
    "            hidden_dim = multiple_of * ((2 * hidden_dim // 3 + multiple_of - 1) // multiple_of)\n",
    "            self.w3 = nn.Linear(hidden_dim, dim)\n",
    "            self.swish = SwishGLU(dim, hidden_dim)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        elif self.mode == 'relu':\n",
    "            self.relu = FeedForwardWithRELU(dim, hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'swish':\n",
    "            return self.dropout(self.w3(self.swish(x)))\n",
    "        elif self.mode == 'relu':\n",
    "            return self.relu(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    feed_forward_mode \"swish\"-前馈神经网络使用swish激活函数，”relu“-使用relu激活函数\n",
    "    norm ：\"RMS\"-使用RMSNorm归一化，其他值则使用nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, n_head, dropout=0.2, feed_forward_mode: str = \"swish\", norm=\"RMS\",\n",
    "                 pos_embed_method=\"repo\"):\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "        self.swish = feed_forward_mode\n",
    "        head_size = n_emb // n_head\n",
    "        self.heads = MultiHeadAttention(n_head, head_size, pos_embed_method=pos_embed_method)\n",
    "        self.fb = FeedForward(n_emb, 4 * n_emb, 2, dropout, mode=feed_forward_mode)\n",
    "        if norm == \"RMS\":\n",
    "            self.l1 = RMSNorm(n_emb)\n",
    "            self.l2 = RMSNorm(n_emb)\n",
    "        else:\n",
    "            self.l1 = nn.LayerNorm(n_emb)\n",
    "            self.l2 = nn.LayerNorm(n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.norm == \"RMS\":\n",
    "            x = self.l1(x)\n",
    "            x = x + self.heads(x)\n",
    "            x = self.l2(x)\n",
    "            x = x + self.fb(x)\n",
    "        else:\n",
    "            x = x + self.heads(x)\n",
    "            x = self.l1(x)\n",
    "            x = x + self.fb(x)\n",
    "            x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockOnlyPERO(nn.Module):\n",
    "    def __init__(self, n_emb, n_head,block_size=256, n_embedding=360, dropout=0.2):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head\n",
    "        self.heads = MultiHeadAttention(n_head, head_size,n_embedding=n_embedding, block_size=block_size)\n",
    "        self.fb = FeedForwardWithRELU(n_emb, n_emb, dropout)\n",
    "        self.l1 = nn.LayerNorm(n_emb)\n",
    "        self.l2 = nn.LayerNorm(n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x + self.heads(x)\n",
    "        x = self.l2(x)\n",
    "        x = x + self.fb(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LLama(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embedding=360, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        # self.position_emb = nn.Embedding(block_size, n_embedding)\n",
    "        self.heads = nn.Sequential(\n",
    "            Block(n_embedding, n_head=4),\n",
    "            Block(n_embedding, n_head=4),\n",
    "            Block(n_embedding, n_head=4),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.l1 = torch.nn.Linear(n_embedding, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        word_embedding = self.embedding(idx)\n",
    "        x = word_embedding\n",
    "        x = self.heads(x)\n",
    "        logits = self.l1(x)\n",
    "        loss = None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size=16):\n",
    "        p = 0\n",
    "        # 生成文本\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            idx_conv = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_conv)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get the probability\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            top_probs = torch.topk(probs, 5, dim=-1)\n",
    "            next_index = torch.multinomial(top_probs[0], num_samples=1)[0][0]\n",
    "            idx_next = top_probs[1][0][next_index.item()].unsqueeze(0).unsqueeze(0)\n",
    "            # idx_next = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "            p = p + torch.log(probs[0][idx_next[0][0]])\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        zhi_xin_du = torch.pow(torch.e, -p / max_new_tokens)\n",
    "        print(f\"置信度：{zhi_xin_du}\")\n",
    "        return idx\n",
    "\n",
    "\n",
    "class LLama1(nn.Module):\n",
    "    \"\"\"\n",
    "    pos_embed_method 限制取值 \"rope\"|\"sin\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, out_features=2, n_heads=4,\n",
    "                 n_embedding=360, block_size=200, dropout=0.2,\n",
    "                 pos_embed_method=\"rope\"):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.out_features = out_features\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embedding = n_embedding\n",
    "        self.pos_embed_method = pos_embed_method\n",
    "        self.block_size = block_size\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        if pos_embed_method != \"rope\":\n",
    "            self.position_emb = nn.Embedding(block_size, n_embedding)\n",
    "        self.heads = nn.Sequential(\n",
    "            Block(n_embedding, n_head=n_heads, pos_embed_method=pos_embed_method),\n",
    "            Block(n_embedding, n_head=n_heads, pos_embed_method=pos_embed_method),\n",
    "            Block(n_embedding, n_head=n_heads, pos_embed_method=pos_embed_method),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.l1 = torch.nn.Linear(n_embedding, out_features)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx\n",
    "        word_embedding = self.embedding(idx)\n",
    "        x = word_embedding\n",
    "        # pos_embed_method==\"sin\" ，使用sinmoid位置编码\n",
    "        if self.pos_embed_method == \"sin\":\n",
    "            pos_embedding = self.position_emb(torch.arange(0, T))\n",
    "            pos_embedding = torch.repeat_interleave(pos_embedding, B)\n",
    "            x = x + pos_embedding\n",
    "        x = self.heads(x)\n",
    "        logits = self.l1(x)\n",
    "        loss = None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits[:, -1, :]\n",
    "            logits = logits.view(B, C)\n",
    "            targets = targets.view(B)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size=16):\n",
    "        # 生成文本\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            idx_conv = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_conv)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get the probability\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            top_probs = torch.topk(probs, 5, dim=-1)\n",
    "            next_index = torch.multinomial(top_probs[0], num_samples=1)[0][0]\n",
    "            idx_next = top_probs[1][0][next_index.item()].unsqueeze(0).unsqueeze(0)\n",
    "            # idx_next = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    def pre(self, idx):\n",
    "        # 预测分类\n",
    "        logits, loss = self(idx)\n",
    "        logits = logits[:, -1, :]\n",
    "        output = torch.argmax(logits, -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LLamaOnlyPERO(nn.Module):\n",
    "    def __init__(self, vocab_size,out_features=2,block_size=256, n_embedding=360, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        self.heads = nn.Sequential(\n",
    "            BlockOnlyPERO(n_embedding, n_head=4),\n",
    "            BlockOnlyPERO(n_embedding, n_head=4),\n",
    "            BlockOnlyPERO(n_embedding, n_head=4),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.l1 = torch.nn.Linear(n_embedding, out_features)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        word_embedding = self.embedding(idx)\n",
    "        x = word_embedding\n",
    "        x = self.heads(x)\n",
    "        logit = self.l1(x)\n",
    "        loss = None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logit.shape\n",
    "            logit = logit[:, -1, :]\n",
    "            logit = logit.view(B, C)\n",
    "            targets = targets.view(B)\n",
    "            loss = torch.nn.functional.cross_entropy(logit, targets)\n",
    "        return logit, loss\n",
    "\n",
    "    def pre(self, idx):\n",
    "        logit, loss = self(idx)\n",
    "        logit = logit[:, -1, :]\n",
    "        output = torch.argmax(logit, -1)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    \n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from typing import Tuple, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):\n",
    "    # 计算词向量元素两两分组之后，每组元素对应的旋转角度\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 生成 token 序列索引 t = [0, 1,..., seq_len-1]\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    # freqs.shape = [seq_len, dim // 2]\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # torch.polar 的文档\n",
    "    # https://pytorch.org/docs/stable/generated/torch.polar.html\n",
    "    # 计算结果是个复数向量\n",
    "    # 假设 freqs = [x, y]\n",
    "    # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "# PERO旋转位置嵌入\n",
    "def apply_rotary_emb(\n",
    "        xq: torch.Tensor,\n",
    "        xk: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # xq.shape = [batch_size, seq_len, dim]\n",
    "    # xq_.shape = [batch_size, seq_len, dim // 2, 2]\n",
    "    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2).to(device)\n",
    "    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2).to(device)\n",
    "\n",
    "    # 转为复数域\n",
    "    xq_ = torch.view_as_complex(xq_)\n",
    "    xk_ = torch.view_as_complex(xk_)\n",
    "    # 应用旋转操作，然后将结果转回实数域\n",
    "    # xq_out.shape = [batch_size, seq_len, dim]\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" 掩藏注意力 \"\"\"\n",
    "    freqs_cis = None\n",
    "\n",
    "    def __init__(self, head_size, n_embedding,block_size, dropout=0.2, pos_embed_method=\"rope\"):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pos_embed_method = pos_embed_method\n",
    "        self.freqs_cis=precompute_freqs_cis(dim=head_size, seq_len=block_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        if self.pos_embed_method == 'rope':\n",
    "            # Reformer相对位置编码\n",
    " #           if self.freqs_cis is None:\n",
    " #               self.freqs_cis = precompute_freqs_cis(dim=key.shape[-1], seq_len=T).to(key.device)\n",
    "            xq, xk = apply_rotary_emb(key, query, self.freqs_cis)\n",
    "            # key*value/(d**-0.5)\n",
    "            query, key = xq, xk\n",
    "        wei = key @ query.transpose(-2, -1) * (key.shape[-1] ** -0.5)\n",
    "        # 掩藏操作，使注意力只能看到前面数据\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, -1)\n",
    "        wei = self.dropout(wei)\n",
    "        value = self.value(x)\n",
    "        outputs = wei @ value\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" 多头注意力 \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, block_size=256,n_embedding=360, pos_embed_method=\"rope\"):\n",
    "        super().__init__()\n",
    "        # 多头注意力由多个注意力叠加\n",
    "        self.heads = nn.ModuleList([Head(head_size, block_size=block_size,n_embedding=n_embedding,pos_embed_method=pos_embed_method) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(n_embedding, n_embedding)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 拼接各个注意力的输出结果\n",
    "        output = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        output = self.dropout(self.linear(output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\" RMSNorm均方层归一化 \"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, eps: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_embedding = n_emb\n",
    "        self.weights = nn.Parameter(torch.ones(n_emb))\n",
    "        self.eps = eps\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x / torch.sqrt(torch.mean(torch.square(x), dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        o = self._norm(x)\n",
    "        return o * self.weights\n",
    "\n",
    "\n",
    "def _Swish(x, beta=1):\n",
    "    return x * (1 / 1 + torch.exp(-beta * x))\n",
    "\n",
    "\n",
    "class SwishGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, beta=1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beta = beta\n",
    "        self.w1 = nn.Linear(dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return _Swish(self.w1(x), self.beta) * self.w2(x)\n",
    "\n",
    "\n",
    "class FeedForwardWithRELU(nn.Module):\n",
    "    \"\"\" 前馈神经网络 \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, hidden_dim, dropout: float):\n",
    "        super().__init__()\n",
    "        self.w3 = nn.Linear(dim, hidden_dim)\n",
    "        self.swish = nn.ReLU()\n",
    "        self.w4 = nn.Linear(hidden_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w4(self.swish(self.w3(x))))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" 前馈神经网络 \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int = 4, dropout: float = 0.2, mode='swish'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        if self.mode == 'swish':\n",
    "            # 4*(2*3*hidden_dim/4) 缩小为2*3倍\n",
    "            hidden_dim = multiple_of * ((2 * hidden_dim // 3 + multiple_of - 1) // multiple_of)\n",
    "            self.w3 = nn.Linear(hidden_dim, dim)\n",
    "            self.swish = SwishGLU(dim, hidden_dim)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        elif self.mode == 'relu':\n",
    "            self.relu = FeedForwardWithRELU(dim, hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'swish':\n",
    "            return self.dropout(self.w3(self.swish(x)))\n",
    "        elif self.mode == 'relu':\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            raise ValueError(\"feed forward mode must is 'swish' or 'relu'\")\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    feed_forward_mode \"swish\"-前馈神经网络使用swish激活函数，”relu“-使用relu激活函数\n",
    "    norm ：\"RMS\"-使用RMSNorm归一化，其他值则使用nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, n_head, block_size,dropout=0.2, feed_forward_mode: str = \"swish\", norm=\"rms\",\n",
    "                 pos_embed_method=\"repo\"):\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "        self.swish = feed_forward_mode\n",
    "        head_size = n_emb // n_head\n",
    "        self.heads = MultiHeadAttention(n_head, head_size, block_size,n_emb,pos_embed_method=pos_embed_method)\n",
    "        self.fb = FeedForward(n_emb, 4 * n_emb, 2, dropout, mode=feed_forward_mode)\n",
    "        if norm == \"rms\":\n",
    "            self.l1 = RMSNorm(n_emb)\n",
    "            self.l2 = RMSNorm(n_emb)\n",
    "        else:\n",
    "            self.l1 = nn.LayerNorm(n_emb)\n",
    "            self.l2 = nn.LayerNorm(n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.norm == \"rms\":\n",
    "            x = self.l1(x)\n",
    "            x = x + self.heads(x)\n",
    "            x = self.l2(x)\n",
    "            x = x + self.fb(x)\n",
    "        else:\n",
    "            x = x + self.heads(x)\n",
    "            x = self.l1(x)\n",
    "            x = x + self.fb(x)\n",
    "            x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockOnlyPERO(nn.Module):\n",
    "    def __init__(self, n_emb, n_head, block_size=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head\n",
    "        self.heads = MultiHeadAttention(n_head, head_size,block_size=block_size,n_embedding=n_emb)\n",
    "        self.fb = FeedForwardWithRELU(n_emb, n_emb, dropout)\n",
    "        self.l1 = nn.LayerNorm(n_emb)\n",
    "        self.l2 = nn.LayerNorm(n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x + self.heads(x)\n",
    "        x = self.l2(x)\n",
    "        x = x + self.fb(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LLama(nn.Module):\n",
    "    #完整的Llama模型\n",
    "    def __init__(self, vocab_size, out_features=2, n_heads=4,\n",
    "                 n_embedding=360, block_size=200, dropout=0.2,\n",
    "                 feed_forward_mode: str = \"swish\", norm=\"rms\",\n",
    "                 pos_embed_method=\"repo\"):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.out_features = out_features\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embedding = n_embedding\n",
    "        self.pos_embed_method = pos_embed_method\n",
    "        self.block_size = block_size\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        if pos_embed_method == \"sin\":\n",
    "            self.position_emb = nn.Embedding(block_size, n_embedding)\n",
    "        self.heads = nn.Sequential(\n",
    "            Block(n_embedding, n_heads,block_size, pos_embed_method=pos_embed_method,feed_forward_mode = feed_forward_mode, norm=norm),\n",
    "            Block(n_embedding, n_heads,block_size, pos_embed_method=pos_embed_method,feed_forward_mode = feed_forward_mode, norm=norm),\n",
    "            Block(n_embedding, n_heads,block_size, pos_embed_method=pos_embed_method,feed_forward_mode = feed_forward_mode, norm=norm),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        #self.l1 = torch.nn.Linear(n_embedding, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        word_embedding = self.embedding(idx)\n",
    "        x = word_embedding\n",
    "        if self.pos_embed_method == \"sin\":\n",
    "            pos_embedding = self.position_emb(torch.arange(0, T))\n",
    "            pos_embedding = torch.repeat_interleave(pos_embedding, B)\n",
    "            x = x + pos_embedding\n",
    "        x = self.heads(x)\n",
    "        return x\n",
    "        # logits = self.l1(x)\n",
    "        # loss = None\n",
    "        # if targets is None:\n",
    "        #     loss = None\n",
    "        # else:\n",
    "        #     B, T, C = logits.shape\n",
    "        #     logits = logits.view(B * T, C)\n",
    "        #     targets = targets.view(B * T)\n",
    "        #     loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        # return logits, loss\n",
    "\n",
    "#     def generate(self, idx, max_new_tokens, block_size=16):\n",
    "#         p = 0\n",
    "#         # 生成文本\n",
    "#         for _ in range(max_new_tokens):\n",
    "#             # get the predictions\n",
    "#             idx_conv = idx[:, -block_size:]\n",
    "#             logits, loss = self(idx_conv)\n",
    "#             logits = logits[:, -1, :]\n",
    "#             # apply softmax to get the probability\n",
    "#             probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "#             top_probs = torch.topk(probs, 5, dim=-1)\n",
    "#             next_index = torch.multinomial(top_probs[0], num_samples=1)[0][0]\n",
    "#             idx_next = top_probs[1][0][next_index.item()].unsqueeze(0).unsqueeze(0)\n",
    "#             # idx_next = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "#             p = p + torch.log(probs[0][idx_next[0][0]])\n",
    "#             idx = torch.cat((idx, idx_next), dim=1)\n",
    "#         zhi_xin_du = torch.pow(torch.e, -p / max_new_tokens)\n",
    "#         print(f\"置信度：{zhi_xin_du}\")\n",
    "#         return idx\n",
    "\n",
    "\n",
    "class LLama1(nn.Module):\n",
    "    \"\"\"\n",
    "    pos_embed_method 限制取值 \"rope\"|\"sin\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, out_features=2, n_heads=4,\n",
    "                 n_embedding=360, block_size=200, dropout=0.2,\n",
    "                 feed_forward_mode: str = \"swish\", norm=\"rms\",\n",
    "                 pos_embed_method=\"repo\"):\n",
    "        super().__init__()\n",
    "        # self.vocab_size = vocab_size\n",
    "        # self.out_features = out_features\n",
    "        # self.n_heads = n_heads\n",
    "        # self.n_embedding = n_embedding\n",
    "        # self.pos_embed_method = pos_embed_method\n",
    "        # self.block_size = block_size\n",
    "        # self.embedding = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        # if pos_embed_method != \"rope\":\n",
    "        #     self.position_emb = nn.Embedding(block_size, n_embedding)\n",
    "        # self.heads = nn.Sequential(\n",
    "        #     Block(n_embedding, n_heads,block_size, pos_embed_method=pos_embed_method),\n",
    "        #     Block(n_embedding, n_heads,block_size, pos_embed_method=pos_embed_method),\n",
    "        #     Block(n_embedding, n_heads,block_size, pos_embed_method=pos_embed_method),\n",
    "        #     nn.Dropout(dropout)\n",
    "        # )\n",
    "        self.l1 = torch.nn.Linear(n_embedding, out_features)\n",
    "        self.llama = LLama(vocab_size, out_features, n_heads,\n",
    "                 n_embedding, block_size, dropout,\n",
    "                 feed_forward_mode,norm,\n",
    "                 pos_embed_method)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # B, T = idx\n",
    "        # word_embedding = self.embedding(idx)\n",
    "        # x = word_embedding\n",
    "        # # pos_embed_method==\"sin\" ，使用sinmoid位置编码\n",
    "        # if self.pos_embed_method == \"sin\":\n",
    "        #     pos_embedding = self.position_emb(torch.arange(0, T))\n",
    "        #     pos_embedding = torch.repeat_interleave(pos_embedding, B)\n",
    "        #     x = x + pos_embedding\n",
    "        # x = self.heads(x)\n",
    "        x = self.llama(idx, targets)\n",
    "        logits = self.l1(x)\n",
    "        loss = None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits[:, -1, :]\n",
    "            logits = logits.view(B, C)\n",
    "            targets = targets.view(B)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size=16):\n",
    "        # 生成文本\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            idx_conv = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_conv)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get the probability\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            top_probs = torch.topk(probs, 5, dim=-1)\n",
    "            next_index = torch.multinomial(top_probs[0], num_samples=1)[0][0]\n",
    "            idx_next = top_probs[1][0][next_index.item()].unsqueeze(0).unsqueeze(0)\n",
    "            # idx_next = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    def pre(self, idx):\n",
    "        # 预测分类\n",
    "        logits, loss = self(idx)\n",
    "        logits = logits[:, -1, :]\n",
    "        output = torch.argmax(logits, -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LLamaOnlyPERO(nn.Module):\n",
    "    def __init__(self, vocab_size,out_features=2,block_size=256, n_embedding=360,n_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        self.heads = nn.Sequential(\n",
    "            BlockOnlyPERO(n_embedding, n_heads,block_size),\n",
    "            BlockOnlyPERO(n_embedding, n_heads,block_size),\n",
    "            BlockOnlyPERO(n_embedding, n_heads,block_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.l1 = torch.nn.Linear(n_embedding, out_features)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        word_embedding = self.embedding(idx)\n",
    "        x = word_embedding\n",
    "        x = self.heads(x)\n",
    "        logit = self.l1(x)\n",
    "        loss = None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logit.shape\n",
    "            logit = logit[:, -1, :]\n",
    "            logit = logit.view(B, C)\n",
    "            targets = targets.view(B)\n",
    "            loss = torch.nn.functional.cross_entropy(logit, targets)\n",
    "        return logit, loss\n",
    "\n",
    "    def pre(self, idx):\n",
    "        logit, loss = self(idx)\n",
    "        logit = logit[:, -1, :]\n",
    "        output = torch.argmax(logit, -1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_mode=\"relu\"\n",
    "norm=\"none\"\n",
    "pos_embed_method=\"repo\"\n",
    "#14分类任务\n",
    "m = LLama1(vocab_size, out_features=14, n_heads=4,\n",
    "                 n_embedding=n_embedding, block_size=block_size, dropout=dropout,\n",
    "                 feed_forward_mode = feed_forward_mode, norm=norm,\n",
    "                 pos_embed_method=pos_embed_method)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "trainLosses = []\n",
    "val_losses = []\n",
    "count = 0\n",
    "logger.info(\"start training\")\n",
    "print(\"start training\")\n",
    "params_num=sum(p.numel() for p in m.parameters()) / 1e6\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "for step in range(max_iter):\n",
    "    m.to(device)\n",
    "    trainLoss = []\n",
    "    logger.info(f\"The step is {step}\")\n",
    "    for X, Y in getABatch('train', batch_size, block_size):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = m(X,Y)\n",
    "        trainLoss.append(loss.item())\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if step != -1:\n",
    "        val_loss = estimate_loss(m)\n",
    "        t_loss = np.mean(trainLoss)\n",
    "        trainLosses.append(t_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        count += 1\n",
    "        logger.info(f\"step{step}: train loss {t_loss}, val loss {val_loss}\")\n",
    "    m.to(\"cpu\")\n",
    "    allCount = 0\n",
    "    accCount = 0\n",
    "    for x, y in getABatch('val', 512, block_size):\n",
    "        x = x.to('cpu')\n",
    "        y = y.to('cpu')\n",
    "        allCount += 512\n",
    "        output = m.pre(x)\n",
    "        y=y.view(block_size)\n",
    "        acc = y == output\n",
    "        acc = acc.sum().item()\n",
    "        accCount += acc\n",
    "    print(f\"step:{step},总数据{allCount}\")\n",
    "    print(f\"准确分类数据{accCount}\")\n",
    "    print(f\"准确率{accCount / allCount}\")\n",
    "    logger.info(f\"step:{step},toal num:{allCount}\")\n",
    "    logger.info(f\"acc num:{accCount}\")\n",
    "    logger.info(f\"acc:{accCount / allCount}\")\n",
    "    torch.save(m,f'./output/mod-_{feed_forward_mode}_{norm}_{pos_embed_method}-{block_size}_{n_embedding}-{step}-{params_num}_{accCount / allCount}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#14分类任务\n",
    "m = LLamaOnlyPERO(vocab_size, out_features=14, block_size=block_size,n_embedding=n_embedding, dropout=dropout)\n",
    "m.to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "trainLosses = []\n",
    "val_losses = []\n",
    "count = 0\n",
    "logger.info(\"start training\")\n",
    "print(\"start training\")\n",
    "for step in range(max_iter):\n",
    "    trainLoss = []\n",
    "    logger.info(f\"The step is {step}\")\n",
    "    for X, Y in getABatch('train', batch_size, block_size):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = m(X,Y)\n",
    "        trainLoss.append(loss.item())\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if step != -1:\n",
    "        val_loss = estimate_loss(m)\n",
    "        t_loss = np.mean(trainLoss)\n",
    "        trainLosses.append(t_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        count += 1\n",
    "        logger.info(f\"step{step}: train loss {t_loss}, val loss {val_loss}\")\n",
    "        print(f\"step{step}: train loss {t_loss}, val loss {val_loss}\")\n",
    "    torch.save(m,f'./output/mod-{step}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m,'./output/mod-last.pth')\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数据83456\n",
      "准确分类数据5489356\n",
      "准确率65.77545053680981\n"
     ]
    }
   ],
   "source": [
    "m.to(\"cpu\")\n",
    "allCount = 0\n",
    "accCount = 0\n",
    "for x, y in getABatch('val', 512, block_size):\n",
    "    x = x.to('cpu')\n",
    "    y = y.to('cpu')\n",
    "    allCount += 512\n",
    "    output = m.pre(x)\n",
    "    acc = y == output\n",
    "    acc = acc.sum().item()\n",
    "    accCount += acc\n",
    "print(f\"总数据{allCount}\")\n",
    "print(f\"准确分类数据{accCount}\")\n",
    "print(f\"准确率{accCount / allCount}\")\n",
    "logger.info(f\"总数据{allCount}\")\n",
    "logger.info(f\"准确分类数据{accCount}\")\n",
    "logger.info(f\"准确率{accCount / allCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  2,  3,  9,  6,  9,  8,  5,  7,  3,  3,  6,  5,  6, 10,  4,  9,  6,\n",
      "        12,  0,  3,  5,  2, 11,  1,  9,  9, 12, 10,  6,  4,  1, 13,  0, 13,  3,\n",
      "        13,  3,  3, 10,  5, 10, 13,  5,  3,  6,  6,  3,  3, 11,  3,  6,  6,  3,\n",
      "         3, 10,  0,  5,  6,  2, 10, 10, 12, 10,  9,  9, 10,  3,  9,  6,  5,  7,\n",
      "         6, 10, 13,  3,  3,  3, 13,  6, 12,  9,  9,  9, 13, 10,  3,  3, 10, 10,\n",
      "         3,  2, 10,  6,  6,  0,  6,  9, 10,  3,  3,  6,  3,  7,  6,  3,  6,  5,\n",
      "         6,  3, 12,  3, 10,  6,  6,  6, 10,  6,  3, 13,  3, 13, 13,  6, 10, 10,\n",
      "         6,  3,  3, 12,  3,  3,  6,  6,  5,  9,  6, 10, 13,  3, 10,  9,  3, 12,\n",
      "         6, 13,  6,  3, 10, 10, 13,  3,  7,  3, 13,  7,  3,  6, 10,  7,  6,  6,\n",
      "         7,  9, 10,  3,  0,  6,  3,  0, 13,  3, 13, 12,  6,  6,  7, 10,  5,  6,\n",
      "         3,  0, 10, 10,  3,  9,  2,  6,  8,  7,  7,  3, 13, 13, 12,  4, 13,  6,\n",
      "        10, 10,  6, 10,  3,  3,  3,  5,  6,  3, 10,  3,  3,  3,  7,  6,  3,  3,\n",
      "         9,  0,  3,  5, 13, 10,  7, 10,  3,  7,  9,  6,  3,  3,  9,  6,  6,  3,\n",
      "        10, 10,  6,  5,  5, 10,  0,  5,  2,  9,  3, 12,  7,  2, 13,  9,  9,  3,\n",
      "         3,  2,  3,  9,  6,  6,  1,  3, 10,  4, 10,  6,  3,  3,  9,  4, 10, 13,\n",
      "         4, 10,  4, 10,  4, 10,  3,  3,  5,  9,  6,  6,  9,  3,  3,  6, 13,  5,\n",
      "         6,  4,  3,  3,  6, 13, 10, 12,  9,  3, 10,  4, 12, 10,  6, 10, 10,  8,\n",
      "         3,  6,  3, 10, 10,  9,  7, 13,  6,  3,  7,  9,  6,  9, 10,  9,  6, 13,\n",
      "         2,  6,  9,  3, 12,  7, 10,  3, 13,  4, 13,  0,  8,  6, 10,  7,  6, 13,\n",
      "         6,  3, 10, 10,  2, 13,  6,  6,  1,  4, 13, 10, 10,  5, 10,  7,  9,  6,\n",
      "         6,  9,  7, 10,  7, 10,  3, 12,  3,  9,  7, 10,  9,  6, 13, 13, 12, 13,\n",
      "         3,  5,  0,  9,  0,  2,  3, 12,  3,  0,  5, 10, 10,  4,  6,  9,  9,  6,\n",
      "         4, 10, 10, 13,  0,  6,  3, 10,  6,  9,  5,  7, 10,  3,  6,  9,  5, 10,\n",
      "        10,  3, 13,  3,  7,  9,  6,  7,  0,  3,  5,  5,  6, 10,  6,  2, 13,  6,\n",
      "        10,  6, 10,  9, 12,  6, 13, 10, 10,  3, 10,  5,  5, 13, 13,  3,  0, 10,\n",
      "        13,  6, 10, 10,  7,  4,  3,  6,  6,  4,  6, 10,  3,  3,  9,  9, 13, 13,\n",
      "         6,  9,  7, 10,  9, 10,  7,  8, 10,  4,  6,  3,  7, 10,  5,  6,  2,  3,\n",
      "         3,  6,  8,  6,  4,  8, 13,  3,  9,  7,  9,  9, 13,  4, 10,  3,  4,  7,\n",
      "         2,  7,  9,  3,  9,  0, 12,  6])\n",
      "tensor([10,  2,  3,  9,  6,  9,  8,  5,  7,  3,  3,  6,  5,  6, 10,  4,  9,  6,\n",
      "        12,  0,  3,  5,  2, 11,  1,  9,  9, 12, 10,  6,  4,  1, 13,  0, 13,  3,\n",
      "        13,  3,  3, 10,  9, 10, 13,  5,  3,  6,  6,  3,  3,  8,  3,  7,  6,  3,\n",
      "         3, 10,  0,  5,  6,  2, 10, 10, 12, 10,  9,  9, 10,  3,  9,  6,  5,  7,\n",
      "         6, 10, 13,  3,  0,  3, 13,  6, 12,  3,  9,  9, 13, 10,  3,  3, 10, 10,\n",
      "         3,  2, 10,  6,  6, 13,  6,  6, 10,  3,  3,  4,  3,  7,  6,  3,  6,  5,\n",
      "         6,  3, 12,  3, 10,  6,  6,  6, 10,  6,  3, 13,  3, 13, 13,  6, 10, 10,\n",
      "         6,  3,  3, 12,  3,  3,  6,  6,  5,  9,  6, 10, 13,  3, 10,  9,  3, 12,\n",
      "         6, 13,  6,  3, 10, 10, 13,  3,  5,  3, 13,  7,  3,  6, 10,  7,  6,  6,\n",
      "         7,  7, 10,  3,  0,  6,  3,  0, 13,  3, 13, 12,  6,  6,  7, 10,  5,  6,\n",
      "         3,  0, 10, 10,  3,  9,  3,  6,  8,  7,  7,  3, 13, 13, 12,  4, 13,  6,\n",
      "        10, 10,  6, 10,  3,  3,  3,  5,  6,  3, 10,  3,  3,  3,  7,  6,  3,  3,\n",
      "         9,  0,  3,  5, 13, 10,  7, 10,  3,  7,  9,  6,  3,  3,  9,  6,  6,  3,\n",
      "        10, 10,  6,  5,  5, 10,  0,  5,  2,  9,  3, 12,  7,  4, 13,  6,  9,  3,\n",
      "         3,  2,  3,  9,  6,  6,  1,  3, 10,  4, 10,  6,  3,  3,  9,  4, 10, 13,\n",
      "         4, 10,  4, 10,  4, 10,  3,  3,  5,  9,  6,  6,  6,  3,  3,  6, 13,  5,\n",
      "         6,  4,  3,  3,  6, 13, 10,  6,  9,  9, 10,  4, 12, 10,  6, 10, 10,  8,\n",
      "         3,  6,  3, 10, 10,  9,  7, 13,  6,  3,  6,  9,  6,  9, 10,  9,  6, 13,\n",
      "         2,  6,  9,  3, 12,  7, 10,  3, 13,  4, 13,  0,  8,  6, 10,  7,  6, 13,\n",
      "         6,  3, 10, 10,  2, 13,  6,  6,  1,  4, 13, 10, 10,  5, 10,  7,  9,  6,\n",
      "         6,  9,  7, 10,  7, 10,  3,  6,  3,  9,  7, 10,  9,  6, 13, 13,  6,  6,\n",
      "         3,  5,  0,  9,  0,  2,  3, 12,  3,  0,  5, 10, 10,  4,  6,  9,  9,  6,\n",
      "         4, 10, 10, 13,  0,  6,  3, 10,  6,  9,  5,  7, 10,  3,  6,  9,  5, 10,\n",
      "        10,  3, 13,  3,  7,  9,  6,  7,  0,  3,  5,  5,  6, 10,  6,  2, 13,  6,\n",
      "        12,  6, 10,  9, 12,  6, 13, 10, 10,  3, 10,  5,  5, 13, 13,  3,  0, 10,\n",
      "        13,  6, 10, 10, 13,  4,  3,  6,  6,  4,  6, 10,  3,  3,  9,  3, 13, 13,\n",
      "         6,  9,  7, 10,  9, 10,  7,  8, 10,  4,  6,  3,  7, 10,  5,  6,  2,  3,\n",
      "         3,  6,  8,  6,  4,  8, 13,  3,  9,  7,  7,  9, 13,  4, 10,  3,  4,  7,\n",
      "         2,  7,  9,  3,  9,  0, 12,  6])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(output)\n",
    "acc=output==y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:5,总数据83456\n",
      "准确分类数据79369\n",
      "准确率0.9510280866564417\n",
      "step:6,总数据83456\n",
      "准确分类数据79224\n",
      "准确率0.9492906441717791\n",
      "step:7,总数据83456\n",
      "准确分类数据79387\n",
      "准确率0.9512437691717791\n",
      "step:8,总数据83456\n",
      "准确分类数据79340\n",
      "准确率0.9506805981595092\n",
      "step:9,总数据83456\n",
      "准确分类数据79411\n",
      "准确率0.9515313458588958\n"
     ]
    }
   ],
   "source": [
    "for step in range(5,10):\n",
    "    m.to(device)\n",
    "    trainLoss = []\n",
    "    logger.info(f\"The step is {step}\")\n",
    "    for X, Y in getABatch('train', batch_size, block_size):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = m(X,Y)\n",
    "        trainLoss.append(loss.item())\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if step != -1:\n",
    "        val_loss = estimate_loss(m)\n",
    "        t_loss = np.mean(trainLoss)\n",
    "        trainLosses.append(t_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        count += 1\n",
    "        logger.info(f\"step{step}: train loss {t_loss}, val loss {val_loss}\")\n",
    "    m.to(\"cpu\")\n",
    "    allCount = 0\n",
    "    accCount = 0\n",
    "    for x, y in getABatch('val', 512, block_size):\n",
    "        x = x.to('cpu')\n",
    "        y = y.to('cpu')\n",
    "        allCount += 512\n",
    "        output = m.pre(x)\n",
    "        y=y.view(block_size)\n",
    "        acc = y == output\n",
    "        acc = acc.sum().item()\n",
    "        accCount += acc\n",
    "    print(f\"step:{step},总数据{allCount}\")\n",
    "    print(f\"准确分类数据{accCount}\")\n",
    "    print(f\"准确率{accCount / allCount}\")\n",
    "    logger.info(f\"step:{step},toal num:{allCount}\")\n",
    "    logger.info(f\"acc num:{accCount}\")\n",
    "    logger.info(f\"acc:{accCount / allCount}\")\n",
    "    torch.save(m,f'./output/mod-_{feed_forward_mode}_{norm}_{pos_embed_method}-{block_size}_{n_embedding}-{step}-{params_num}_{accCount / allCount}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQAHead(nn.Module):\n",
    "    \"\"\" 掩藏注意力 \"\"\"\n",
    "    freqs_cis = None\n",
    "\n",
    "    def __init__(self, head_size, n_embedding, block_size, dropout=0.2, pos_embed_method=\"rope\"):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pos_embed_method = pos_embed_method\n",
    "        self.freqs_cis = precompute_freqs_cis(dim=head_size, seq_len=block_size).to(device)\n",
    "\n",
    "    def forward(self, x, key, value):\n",
    "        B, T, C = x.shape\n",
    "        query = self.query(x)\n",
    "        if self.pos_embed_method == 'rope':\n",
    "            # Reformer相对位置编码\n",
    "            #           if self.freqs_cis is None:\n",
    "            #               self.freqs_cis = precompute_freqs_cis(dim=key.shape[-1], seq_len=T).to(key.device)\n",
    "            xq, xk = apply_rotary_emb(key, query, self.freqs_cis)\n",
    "            # key*value/(d**-0.5)\n",
    "            query, key = xq, xk\n",
    "        wei = key @ query.transpose(-2, -1) * (key.shape[-1] ** -0.5)\n",
    "        # 掩藏操作，使注意力只能看到前面数据\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, -1)\n",
    "        wei = self.dropout(wei)\n",
    "        outputs = wei @ value\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\" 多组查询注意力 \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, block_size=256, n_embedding=360, pos_embed_method=\"rope\"):\n",
    "        super().__init__()\n",
    "        # 多头注意力由多个注意力叠加\n",
    "        self.heads = nn.ModuleList(\n",
    "            [GQAHead(head_size=head_size, block_size=block_size, n_embedding=n_embedding,\n",
    "                     pos_embed_method=pos_embed_method) for _ in\n",
    "             range(num_heads)])\n",
    "        self.linear = nn.Linear(n_embedding, n_embedding)\n",
    "        self.key = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 拼接各个注意力的输出结果\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        output = torch.cat([h(x, key, value) for h in self.heads], dim=-1)\n",
    "        output = self.dropout(self.linear(output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class GQABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    feed_forward_mode \"swish\"-前馈神经网络使用swish激活函数，”relu“-使用relu激活函数\n",
    "    norm ：\"RMS\"-使用RMSNorm归一化，其他值则使用nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, n_head, block_size, dropout=0.2, feed_forward_mode: str = \"swish\", norm=\"rms\",\n",
    "                 pos_embed_method=\"repo\"):\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "        self.swish = feed_forward_mode\n",
    "        head_size = n_emb // n_head\n",
    "        self.heads = MultiQueryAttention(n_head, head_size, block_size, n_emb, pos_embed_method=pos_embed_method)\n",
    "        self.fb = FeedForward(n_emb, 4 * n_emb, 2, dropout, mode=feed_forward_mode)\n",
    "        if norm == \"rms\":\n",
    "            self.l1 = RMSNorm(n_emb)\n",
    "            self.l2 = RMSNorm(n_emb)\n",
    "        else:\n",
    "            self.l1 = nn.LayerNorm(n_emb)\n",
    "            self.l2 = nn.LayerNorm(n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.norm == \"rms\":\n",
    "            x = self.l1(x)\n",
    "            x = x + self.heads(x)\n",
    "            x = self.l2(x)\n",
    "            x = x + self.fb(x)\n",
    "        else:\n",
    "            x = x + self.heads(x)\n",
    "            x = self.l1(x)\n",
    "            x = x + self.fb(x)\n",
    "            x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GQALLama(nn.Module):\n",
    "    # 完整的Llama模型\n",
    "    def __init__(self, vocab_size, out_features=2, n_heads=4,\n",
    "                 n_embedding=360, block_size=200, dropout=0.2,\n",
    "                 feed_forward_mode: str = \"swish\", norm=\"rms\",\n",
    "                 pos_embed_method=\"repo\"):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.out_features = out_features\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embedding = n_embedding\n",
    "        self.pos_embed_method = pos_embed_method\n",
    "        self.block_size = block_size\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        if pos_embed_method == \"sin\":\n",
    "            self.position_emb = nn.Embedding(block_size, n_embedding)\n",
    "        self.heads = nn.Sequential(\n",
    "            GQABlock(n_embedding, n_heads, block_size, pos_embed_method=pos_embed_method,\n",
    "                     feed_forward_mode=feed_forward_mode, norm=norm),\n",
    "            GQABlock(n_embedding, n_heads, block_size, pos_embed_method=pos_embed_method,\n",
    "                     feed_forward_mode=feed_forward_mode, norm=norm),\n",
    "            GQABlock(n_embedding, n_heads, block_size, pos_embed_method=pos_embed_method,\n",
    "                     feed_forward_mode=feed_forward_mode, norm=norm),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # self.l1 = torch.nn.Linear(n_embedding, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        word_embedding = self.embedding(idx)\n",
    "        x = word_embedding\n",
    "        if self.pos_embed_method == \"sin\":\n",
    "            pos_embedding = self.position_emb(torch.arange(0, T))\n",
    "            pos_embedding = torch.repeat_interleave(pos_embedding, B)\n",
    "            x = x + pos_embedding\n",
    "        x = self.heads(x)\n",
    "        return x\n",
    "        # logits = self.l1(x)\n",
    "        # loss = None\n",
    "        # if targets is None:\n",
    "        #     loss = None\n",
    "        # else:\n",
    "        #     B, T, C = logits.shape\n",
    "        #     logits = logits.view(B * T, C)\n",
    "        #     targets = targets.view(B * T)\n",
    "        #     loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        # return logits, loss\n",
    "        \n",
    "        \n",
    "class GQALLama1(nn.Module):\n",
    "    \"\"\"\n",
    "    pos_embed_method 限制取值 \"rope\"|\"sin\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, out_features=2, n_heads=4,\n",
    "                 n_embedding=360, block_size=200, dropout=0.2,\n",
    "                 feed_forward_mode: str = \"swish\", norm=\"rms\",\n",
    "                 pos_embed_method=\"repo\"):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(n_embedding, out_features)\n",
    "        self.llama = GQALLama(vocab_size, out_features, n_heads,\n",
    "                              n_embedding, block_size, dropout,\n",
    "                              feed_forward_mode, norm,\n",
    "                              pos_embed_method)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # B, T = idx\n",
    "        # word_embedding = self.embedding(idx)\n",
    "        # x = word_embedding\n",
    "        # # pos_embed_method==\"sin\" ，使用sinmoid位置编码\n",
    "        # if self.pos_embed_method == \"sin\":\n",
    "        #     pos_embedding = self.position_emb(torch.arange(0, T))\n",
    "        #     pos_embedding = torch.repeat_interleave(pos_embedding, B)\n",
    "        #     x = x + pos_embedding\n",
    "        # x = self.heads(x)\n",
    "        x = self.llama(idx, targets)\n",
    "        logits = self.l1(x)\n",
    "        loss = None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits[:, -1, :]\n",
    "            logits = logits.view(B, C)\n",
    "            targets = targets.view(B)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size=16):\n",
    "        # 生成文本\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            idx_conv = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_conv)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get the probability\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            top_probs = torch.topk(probs, 5, dim=-1)\n",
    "            next_index = torch.multinomial(top_probs[0], num_samples=1)[0][0]\n",
    "            idx_next = top_probs[1][0][next_index.item()].unsqueeze(0).unsqueeze(0)\n",
    "            # idx_next = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    def pre(self, idx):\n",
    "        # 预测分类\n",
    "        logits, loss = self(idx)\n",
    "        logits = logits[:, -1, :]\n",
    "        output = torch.argmax(logits, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_mode=\"relu\"\n",
    "norm=\"none\"\n",
    "pos_embed_method=\"repo\"\n",
    "#14分类任务\n",
    "m = GQALLama(vocab_size, out_features=14, n_heads=4,\n",
    "                 n_embedding=n_embedding, block_size=block_size, dropout=dropout,\n",
    "                 feed_forward_mode = feed_forward_mode, norm=norm,\n",
    "                 pos_embed_method=pos_embed_method)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "trainLosses = []\n",
    "val_losses = []\n",
    "count = 0\n",
    "logger.info(\"start training\")\n",
    "print(\"start training\")\n",
    "params_num=sum(p.numel() for p in m.parameters()) / 1e6\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "for step in range(max_iter):\n",
    "    m.to(device)\n",
    "    trainLoss = []\n",
    "    logger.info(f\"The step is {step}\")\n",
    "    for X, Y in getABatch('train', batch_size, block_size):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = m(X,Y)\n",
    "        trainLoss.append(loss.item())\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if step != -1:\n",
    "        val_loss = estimate_loss(m)\n",
    "        t_loss = np.mean(trainLoss)\n",
    "        trainLosses.append(t_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        count += 1\n",
    "        logger.info(f\"step{step}: train loss {t_loss}, val loss {val_loss}\")\n",
    "    m.to(\"cpu\")\n",
    "    allCount = 0\n",
    "    accCount = 0\n",
    "    for x, y in getABatch('val', 512, block_size):\n",
    "        x = x.to('cpu')\n",
    "        y = y.to('cpu')\n",
    "        allCount += 512\n",
    "        output = m.pre(x)\n",
    "        y=y.view(block_size)\n",
    "        acc = y == output\n",
    "        acc = acc.sum().item()\n",
    "        accCount += acc\n",
    "    print(f\"step:{step},总数据{allCount}\")\n",
    "    print(f\"准确分类数据{accCount}\")\n",
    "    print(f\"准确率{accCount / allCount}\")\n",
    "    logger.info(f\"step:{step},toal num:{allCount}\")\n",
    "    logger.info(f\"acc num:{accCount}\")\n",
    "    logger.info(f\"acc:{accCount / allCount}\")\n",
    "    torch.save(m,f'./output/GQA-qkv011-mod-_{feed_forward_mode}_{norm}_{pos_embed_method}-{block_size}_{n_embedding}-{step}-{params_num}_{accCount / allCount}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LY",
   "language": "python",
   "name": "ly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
